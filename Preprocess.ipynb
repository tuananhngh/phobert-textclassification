{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c16c0a5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:16:21.349030Z",
     "start_time": "2021-10-24T09:16:20.384563Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "#from dateparser.search import search_dates\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from vncorenlp import VnCoreNLP\n",
    "from collections import defaultdict, Counter\n",
    "from transformers import AutoModel, AutoTokenizer, BertPreTrainedModel, RobertaModel, RobertaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b85d1c46",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:16:21.938354Z",
     "start_time": "2021-10-24T09:16:21.882810Z"
    }
   },
   "outputs": [],
   "source": [
    "text = {}\n",
    "with open(\"./data.txt\", encoding='utf-8-sig') as f:\n",
    "    text = json.loads(f.read())\n",
    "\n",
    "seq = [text[\"data_direction\"][i][\"subject\"] for i in range(len(text[\"data_direction\"]))]\n",
    "lab = [text[\"data_direction\"][i][\"category\"] for i in range(len(text[\"data_direction\"]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064ef418",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:16:22.134182Z",
     "start_time": "2021-10-24T09:16:22.117476Z"
    }
   },
   "outputs": [],
   "source": [
    "data_pd = pd.DataFrame({\"Sequence\": seq, \"Label\": lab})\n",
    "\n",
    "acrronym = {\"NQ\": \"Nghị quyết\", \"CP\": \"Chính phủ\", \"TTTT\": \"Thông tin truyền thông\",\n",
    "            \"CBCCVCLĐ\" : \"Cán bộ công chức viên chức người lao động\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db52b753",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:24:53.563380Z",
     "start_time": "2021-10-24T09:24:53.545429Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_accronym(data_file):\n",
    "    accronym = []\n",
    "    for i in range(data_file.shape[0]):\n",
    "        sent = data_file.iloc[i][\"Sequence\"]\n",
    "        sent_acc = re.findall(r'[A-ZĐ]{2,}', sent)\n",
    "        if sent_acc:\n",
    "            for ele in sent_acc:\n",
    "                accronym.append(ele)\n",
    "    return set(accronym)\n",
    "\n",
    "def regex_sentence(s):\n",
    "    s = re.sub('((www\\.[^s]+)|(https://[^\\s]+))', 'URL', s)  # replace url\n",
    "    s = re.sub(\"V/v\", \"\", s)\n",
    "    s = re.sub(\"v/v\", \"\", s)\n",
    "    s = re.sub(\"Về việc\", \"\", s)\n",
    "    s = re.sub(r'[-–()/\"#@;:<>{}`+=~|.!?,&“”%*⋅…]', ' ', s)\n",
    "    s = re.sub(r\"\\b\\d+\\b\", '', s)  # remove number, date, etc...\n",
    "    #s = re.sub(\"TTg\", \"\", s)\n",
    "    #s = re.sub(\"CTr]\", \"\", s)\n",
    "    #s = re.sub(r'\\b[A-ZÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠ]\\b', \"\", s)  #remove single uppercase character\n",
    "    s = re.sub(r'\\b[BCEXHVICJFQPKcvhđmgbs]\\b',\"\",s)\n",
    "    #s = re.sub(r'[A-ZÀÁÂÃÈÉÊÌÍÒÓÔÕÙÚĂĐĨŨƠ]{2,}', \"\", s)  #remove 2 consecutive uppercase character\n",
    "    #s = re.sub('[\\n]+', '', s)  #remove white space\n",
    "    s = s.replace('\\n', '').replace('\\r', '').replace(\"\\\\\", \"\")\n",
    "    s = s.strip()\n",
    "    s = ' '.join(word for word in s.split())  #\n",
    "    s = s.lower()\n",
    "    return s\n",
    "\n",
    "\n",
    "def get_single_letter(s):\n",
    "    return [word for word in s.split() if len(word)==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a1fc24a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:24:54.537618Z",
     "start_time": "2021-10-24T09:24:54.194968Z"
    }
   },
   "outputs": [],
   "source": [
    "clean_data = data_pd.copy()\n",
    "clean_data[\"Sequence\"] = clean_data[\"Sequence\"].apply(regex_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19d6859e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:16:23.093843Z",
     "start_time": "2021-10-24T09:16:23.036175Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ý', 'y', 'ở', 'ô', 'á', 'e', 'a', 'ạ', 'ỷ', 'i'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def single_letters(data):\n",
    "    single = defaultdict(list)\n",
    "    for i,seq in enumerate(data[\"Sequence\"]):\n",
    "        ok = get_single_letter(seq)\n",
    "        for lt in ok:\n",
    "            single[lt].append(i)\n",
    "    return single\n",
    "\n",
    "single = single_letters(clean_data)\n",
    "\n",
    "single.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e899a413",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:16:28.736294Z",
     "start_time": "2021-10-24T09:16:23.319065Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_unique_label(dataset):\n",
    "    dict_text = defaultdict(list)\n",
    "    for k, v in zip(dataset[\"Sequence\"], dataset[\"Label\"]):\n",
    "        if v not in dict_text[k]:\n",
    "            #print(f\"{v} not in {dict_text[k]}\")\n",
    "            dict_text[k].append(v)\n",
    "        else:\n",
    "            #print(f\"{v} in {dict_text[k]}\")\n",
    "            pass\n",
    "    return dict_text\n",
    "\n",
    "rdrsegmenter = VnCoreNLP(\"/Users/Slaton/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m')\n",
    "\n",
    "def segmenter(s):\n",
    "    sentence = rdrsegmenter.tokenize(s)[0]\n",
    "    return \" \".join(sentence)\n",
    "\n",
    "def data_label_dict(dataframe):\n",
    "    unseg_dict = get_unique_label(dataframe)\n",
    "    seg_dict = dict()  #defaultdict(list)\n",
    "    for k, v in unseg_dict.items():\n",
    "        seg_dict[segmenter(k)] = v\n",
    "    return seg_dict\n",
    "\n",
    "def create_pd_dummies_label(dictionary, label_list):\n",
    "    empty_pd = pd.DataFrame(index=range(len(dictionary)), columns=[\"Sequence\", *label_list])\n",
    "    for i, k in enumerate(dictionary.keys()):\n",
    "        empty_pd.iloc[i][\"Sequence\"] = k\n",
    "        for lab in dictionary[k]:\n",
    "            empty_pd.iloc[i][lab] = 1\n",
    "    empty_pd = empty_pd.fillna(0)\n",
    "    return empty_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7aab9d0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:16:52.624133Z",
     "start_time": "2021-10-24T09:16:28.737453Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = list(np.unique(clean_data[\"Label\"]))\n",
    "segmented_dict = data_label_dict(clean_data)\n",
    "vocab = set([word for k in segmented_dict.keys() for word in k.split()])  #3938"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6bc4520",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:16:53.059185Z",
     "start_time": "2021-10-24T09:16:52.624817Z"
    }
   },
   "outputs": [],
   "source": [
    "final_data = create_pd_dummies_label(segmented_dict, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6e34168",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:16:53.068328Z",
     "start_time": "2021-10-24T09:16:53.060519Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequence</th>\n",
       "      <th>Báo chí xuất bản</th>\n",
       "      <th>Báo cáo</th>\n",
       "      <th>Chỉ thị</th>\n",
       "      <th>Công văn</th>\n",
       "      <th>Giấy mời</th>\n",
       "      <th>Hướng dẫn</th>\n",
       "      <th>Kế hoạch</th>\n",
       "      <th>Quyết định</th>\n",
       "      <th>Thông báo</th>\n",
       "      <th>Thông tư</th>\n",
       "      <th>Tờ trình</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>thống_kê danh_sách cá_nhân gia_đình hiến máu t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xin cấp tên_miền cho trang thông_tin điện_tử t...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>đề_nghị hỗ_trợ tập_huấn triển_khai ứng_dụng ch...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>thống_kê chỉ_tiêu theo nghị_quyết nq cp ngày s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vận_động cbccvclđ tham_gia hiến máu tình_nguyệ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10135</th>\n",
       "      <td>phân_công nhiệm_vụ chủ_tịch các phó chủ_tịch u...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10136</th>\n",
       "      <td>tham_gia ý_kiến đối_với dự_thảo xin chủ_trương...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10137</th>\n",
       "      <td>tiếp_tục thực_hiện một_số biện_pháp cấp_bách p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10138</th>\n",
       "      <td>phúc_đáp công_văn số stttt bcvt cntt ngày của ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10139</th>\n",
       "      <td>góp_ý chủ_trương thí_điểm hạ ngầm cáp viễn_thô...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10140 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Sequence  Báo chí xuất bản  \\\n",
       "0      thống_kê danh_sách cá_nhân gia_đình hiến máu t...                 0   \n",
       "1      xin cấp tên_miền cho trang thông_tin điện_tử t...                 0   \n",
       "2      đề_nghị hỗ_trợ tập_huấn triển_khai ứng_dụng ch...                 0   \n",
       "3      thống_kê chỉ_tiêu theo nghị_quyết nq cp ngày s...                 0   \n",
       "4      vận_động cbccvclđ tham_gia hiến máu tình_nguyệ...                 0   \n",
       "...                                                  ...               ...   \n",
       "10135  phân_công nhiệm_vụ chủ_tịch các phó chủ_tịch u...                 0   \n",
       "10136  tham_gia ý_kiến đối_với dự_thảo xin chủ_trương...                 0   \n",
       "10137  tiếp_tục thực_hiện một_số biện_pháp cấp_bách p...                 0   \n",
       "10138  phúc_đáp công_văn số stttt bcvt cntt ngày của ...                 0   \n",
       "10139  góp_ý chủ_trương thí_điểm hạ ngầm cáp viễn_thô...                 0   \n",
       "\n",
       "       Báo cáo  Chỉ thị  Công văn  Giấy mời  Hướng dẫn  Kế hoạch  Quyết định  \\\n",
       "0            0        0         1         0          0         0           0   \n",
       "1            0        0         1         0          0         0           0   \n",
       "2            0        0         1         0          0         0           0   \n",
       "3            0        0         1         0          0         0           0   \n",
       "4            0        0         1         0          0         0           0   \n",
       "...        ...      ...       ...       ...        ...       ...         ...   \n",
       "10135        0        0         0         0          0         0           1   \n",
       "10136        0        0         1         0          0         0           0   \n",
       "10137        0        0         1         0          0         0           0   \n",
       "10138        0        0         1         0          0         0           0   \n",
       "10139        0        0         1         0          0         0           0   \n",
       "\n",
       "       Thông báo  Thông tư  Tờ trình  \n",
       "0              0         0         0  \n",
       "1              0         0         0  \n",
       "2              0         0         0  \n",
       "3              0         0         0  \n",
       "4              0         0         0  \n",
       "...          ...       ...       ...  \n",
       "10135          0         0         0  \n",
       "10136          0         0         0  \n",
       "10137          0         0         0  \n",
       "10138          0         0         0  \n",
       "10139          0         0         0  \n",
       "\n",
       "[10140 rows x 12 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21b7db7",
   "metadata": {},
   "source": [
    "# Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88f512e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:25:04.711852Z",
     "start_time": "2021-10-24T09:25:00.007769Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ef08e201",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T14:06:57.320059Z",
     "start_time": "2021-10-24T14:06:57.315658Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import compute_sample_weight\n",
    "weight = 1/np.sum(y_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "9dafcbde",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T14:06:34.911910Z",
     "start_time": "2021-10-24T14:06:34.903876Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  65,  573,   46, 4240,  582,   41,  316,  746,  411,   48,   92])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y_train,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfd90fd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:25:13.205132Z",
     "start_time": "2021-10-24T09:25:13.188965Z"
    }
   },
   "outputs": [],
   "source": [
    "labels = final_data[[lab for lab in final_data.columns if not lab.startswith(\"Sequence\")]].values\n",
    "Xfinal = final_data[\"Sequence\"].values\n",
    "X_train, X_val, y_train, y_val = train_test_split(Xfinal, labels, test_size=0.3, random_state=0)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8720f68e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:25:18.432096Z",
     "start_time": "2021-10-24T09:25:17.715021Z"
    }
   },
   "outputs": [],
   "source": [
    "encoded_cv = [tokenizer.encode(sent, add_special_tokens=True) for sent in final_data[\"Sequence\"]]\n",
    "max_len = max([len(sent) for sent in encoded_cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1050912e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:25:18.916802Z",
     "start_time": "2021-10-24T09:25:18.911618Z"
    }
   },
   "outputs": [],
   "source": [
    "def bert_preprocessing(data):\n",
    "    inputs_id = []\n",
    "    att_masks = []\n",
    "    for sent in data:\n",
    "        encoded_sent = tokenizer.encode_plus(text=sent, add_special_tokens=True, return_attention_mask=True,max_length=114,padding='max_length',)\n",
    "        #print(len(encoded_sent.get('input_ids')))\n",
    "        inputs_id.append(encoded_sent.get('input_ids'))\n",
    "        att_masks.append(encoded_sent.get('attention_mask'))\n",
    "    inputs_id = torch.tensor(inputs_id)\n",
    "    att_masks = torch.tensor(att_masks)\n",
    "    return inputs_id, att_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e70b2edc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:25:21.252260Z",
     "start_time": "2021-10-24T09:25:20.527990Z"
    }
   },
   "outputs": [],
   "source": [
    "train_inputs_id, train_mask = bert_preprocessing(X_train)\n",
    "val_inputs_id, val_mask = bert_preprocessing(X_val)\n",
    "test_inputs_id, test_mask = bert_preprocessing(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d2d63c",
   "metadata": {},
   "source": [
    "# DataLoader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fcf7a8d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:25:21.996858Z",
     "start_time": "2021-10-24T09:25:21.993784Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c184ce5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:25:22.962280Z",
     "start_time": "2021-10-24T09:25:22.955738Z"
    }
   },
   "outputs": [],
   "source": [
    "train_lab = torch.tensor(y_train,dtype=torch.float)\n",
    "val_lab = torch.tensor(y_val,dtype=torch.float)\n",
    "test_lab = torch.tensor(y_test, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a8284d1b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T09:25:23.397716Z",
     "start_time": "2021-10-24T09:25:23.388835Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_data = TensorDataset(train_inputs_id, train_mask, train_lab)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_loader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "val_data = TensorDataset(val_inputs_id, val_mask, val_lab)\n",
    "val_sampler = RandomSampler(val_data)\n",
    "val_loader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)\n",
    "\n",
    "test_data = TensorDataset(test_inputs_id, test_mask, test_lab)\n",
    "test_sampler = RandomSampler(test_data)\n",
    "test_loader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d98ab9",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7238935b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T14:12:09.068582Z",
     "start_time": "2021-10-24T14:12:09.056740Z"
    }
   },
   "outputs": [],
   "source": [
    "class PhoBertClassifier(nn.Module):\n",
    "    def __init__(self, freeze=True):\n",
    "        super(PhoBertClassifier,self).__init__()\n",
    "        d_in, hidden, d_out = 768, 64, 11\n",
    "        self.bert = AutoModel.from_pretrained(\"vinai/phobert-base\")\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(d_in, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, d_out))\n",
    "        self.sgm = nn.Sigmoid()\n",
    "        if freeze:\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "    def forward(self, inp_id, att_msk):\n",
    "        output = self.bert(input_ids = inp_id, attention_mask = att_msk)\n",
    "        last_hidden = output[0][:,0,:]\n",
    "        output_cls = self.classifier(last_hidden)\n",
    "        output_sgm = self.sgm(output_cls)\n",
    "        return output_sgm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b37ec649",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T14:12:09.753284Z",
     "start_time": "2021-10-24T14:12:09.748869Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "def initialize_model(epochs=4):\n",
    "    phobert = PhoBertClassifier()\n",
    "    optimizer = AdamW(phobert.parameters(),\n",
    "                     lr=3e-5,\n",
    "                     eps=1e-8)\n",
    "    total_steps = len(train_loader)*epochs\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer=optimizer,\n",
    "                                               num_warmup_steps=0,\n",
    "                                               num_training_steps=total_steps)\n",
    "    return phobert, optimizer, scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ee42cbf0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T14:12:10.561993Z",
     "start_time": "2021-10-24T14:12:10.535072Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "loss_fn = nn.BCELoss(reduction=\"none\")\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    torch.cuda.manual_seed_all(seed_value)\n",
    "    \n",
    "def train(model,optimizer,scheduler,train_dataloader, val_dataloader, epochs=4,evaluation=False):\n",
    "    # Start training loop\n",
    "    print(\"Start training...\\n\")\n",
    "    for epoch_i in range(epochs):\n",
    "        # =======================================\n",
    "        #               Training\n",
    "        # =======================================\n",
    "        # Print the header of the result table\n",
    "        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val F1':^9} | {'Val Precision':^9} | {'Val Recall':^9}\")\n",
    "        #print(\"-\"*70)\n",
    "        #print(f\"----------Epoch {epoch_i}----------\")\n",
    "        # Measure the elapsed time of each epoch\n",
    "        t0_epoch, t0_batch = time.time(), time.time()\n",
    "        # Reset tracking variables at the beginning of each epoch\n",
    "        total_loss, batch_loss, batch_counts = 0, 0, 0\n",
    "        model.train()\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            batch_counts+=1\n",
    "            b_input_id, b_att_mask, b_label = batch\n",
    "            #model.zero_grad()\n",
    "            \n",
    "            output = model(b_input_id, b_att_mask)\n",
    "            tmp_loss = loss_fn(output, b_label)\n",
    "            weighted_loss = tmp_loss*torch.tensor(weight)\n",
    "            loss = weighted_loss.mean()\n",
    "            \n",
    "            batch_loss += loss.item()\n",
    "            total_loss += loss.item()\n",
    "            with torch.enable_grad():\n",
    "                loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            #scheduler.step()\n",
    "            \n",
    "            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n",
    "                # Calculate time elapsed for 20 batches\n",
    "                time_elapsed = time.time() - t0_batch\n",
    "                \n",
    "                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss / batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {'-':^10} | {'-':^10}\")\n",
    "                \n",
    "                batch_loss, batch_counts = 0, 0\n",
    "                t0_batch = time.time()\n",
    "            avg_train_loss = total_loss/len(train_dataloader)\n",
    "            #print(\"-\"*70)\n",
    "            # =======================================\n",
    "            #               Evaluation\n",
    "            # =======================================\n",
    "        if evaluation == True:\n",
    "            # After the completion of each training epoch, measure the model's performance\n",
    "            # on our validation set.\n",
    "            #val_loss, val_accuracy = evaluate(model, val_dataloader)\n",
    "            val_loss, val_f1, val_precision, val_recall = evaluate(model, val_dataloader)\n",
    "\n",
    "            # Print performance over the entire training data\n",
    "            time_elapsed = time.time() - t0_epoch\n",
    "            \n",
    "            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_f1:^10.6f} | {val_precision:^10.6f} | {val_recall:^10.6f}\")\n",
    "                #print(\"-\"*70)\n",
    "        print(\"\\n\")\n",
    "    \n",
    "    print(\"Training complete!\")\n",
    "    \n",
    "def calculate_metrics(pred, target, threshold=0.5):\n",
    "    pred = np.array(pred > threshold, dtype=float)\n",
    "    return {'micro/precision': precision_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'micro/recall': recall_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'micro/f1': f1_score(y_true=target, y_pred=pred, average='micro'),\n",
    "            'macro/precision': precision_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'macro/recall': recall_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'macro/f1': f1_score(y_true=target, y_pred=pred, average='macro'),\n",
    "            'samples/precision': precision_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            'samples/recall': recall_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            'samples/f1': f1_score(y_true=target, y_pred=pred, average='samples'),\n",
    "            }\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate(model, val_dataloader):\n",
    "    \"\"\"After the completion of each training epoch, measure the model's performance\n",
    "    on our validation set.\n",
    "    \"\"\"\n",
    "    # Put the model into the evaluation mode. The dropout layers are disabled during\n",
    "    # the test time.\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables\n",
    "    #val_accuracy = []\n",
    "    val_loss = []\n",
    "    val_f1, val_prec, val_recall = [], [], []\n",
    "\n",
    "    # For each batch in our validation set...\n",
    "    for batch in val_dataloader:\n",
    "        # Load batch to GPU\n",
    "        b_input_ids, b_attn_mask, b_labels = batch #tuple(t.to(device) for t in batch)\n",
    "\n",
    "        # Compute logits\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, b_attn_mask)\n",
    "\n",
    "        # Compute loss\n",
    "        tmp_loss = loss_fn(logits, b_labels)\n",
    "        weighted_loss = tmp_loss*torch.tensor(weight)\n",
    "        loss = weighted_loss.mean()\n",
    "        val_loss.append(loss.item())\n",
    "\n",
    "        # Get the predictions\n",
    "        #preds = torch.argmax(logits, dim=1).flatten()\n",
    "        preds = np.array(logits>0.5, dtype=float)\n",
    "\n",
    "        # Calculate the accuracy rate\n",
    "        #ccuracy = (preds == b_labels).cpu().numpy().mean() * 100\n",
    "        precision = precision_score(b_labels.numpy(), preds,average='weighted', zero_division=0)\n",
    "        f1 = f1_score(b_labels.numpy(), preds,average='weighted', zero_division=0)\n",
    "        recall = recall_score(b_labels.numpy(), preds,average='weighted', zero_division=0)\n",
    "        \n",
    "        val_f1.append(f1)\n",
    "        val_recall.append(recall)\n",
    "        val_prec.append(precision)\n",
    "\n",
    "    # Compute the average accuracy and loss over the validation set.\n",
    "    val_loss = np.mean(val_loss)\n",
    "    val_f1 = np.mean(val_f1)\n",
    "    val_prec = np.mean(val_prec)\n",
    "    val_recall = np.mean(val_recall)\n",
    "    #val_accuracy = np.mean(val_accuracy)\n",
    "\n",
    "    return val_loss, val_f1, val_prec, val_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "a4467970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T18:11:29.689066Z",
     "start_time": "2021-10-24T14:12:19.102829Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val F1   | Val Precision | Val Recall\n",
      "   1    |   20    |   0.006196   |     -      |     -     |     -      |     -     \n",
      "   1    |   40    |   0.005848   |     -      |     -     |     -      |     -     \n",
      "   1    |   60    |   0.005520   |     -      |     -     |     -      |     -     \n",
      "   1    |   80    |   0.005153   |     -      |     -     |     -      |     -     \n",
      "   1    |   100   |   0.004783   |     -      |     -     |     -      |     -     \n",
      "   1    |   120   |   0.004402   |     -      |     -     |     -      |     -     \n",
      "   1    |   140   |   0.004074   |     -      |     -     |     -      |     -     \n",
      "   1    |   160   |   0.003746   |     -      |     -     |     -      |     -     \n",
      "   1    |   180   |   0.003481   |     -      |     -     |     -      |     -     \n",
      "   1    |   200   |   0.003245   |     -      |     -     |     -      |     -     \n",
      "   1    |   220   |   0.002964   |     -      |     -     |     -      |     -     \n",
      "   1    |   221   |   0.002908   |     -      |     -     |     -      |     -     \n",
      "   1    |    -    |   0.004493   |  0.002572  |  0.257844  |  0.339947  |  0.214286 \n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val F1   | Val Precision | Val Recall\n",
      "   2    |   20    |   0.002743   |     -      |     -     |     -      |     -     \n",
      "   2    |   40    |   0.002530   |     -      |     -     |     -      |     -     \n",
      "   2    |   60    |   0.002350   |     -      |     -     |     -      |     -     \n",
      "   2    |   80    |   0.002222   |     -      |     -     |     -      |     -     \n",
      "   2    |   100   |   0.002087   |     -      |     -     |     -      |     -     \n",
      "   2    |   120   |   0.001908   |     -      |     -     |     -      |     -     \n",
      "   2    |   140   |   0.001879   |     -      |     -     |     -      |     -     \n",
      "   2    |   160   |   0.001678   |     -      |     -     |     -      |     -     \n",
      "   2    |   180   |   0.001618   |     -      |     -     |     -      |     -     \n",
      "   2    |   200   |   0.001460   |     -      |     -     |     -      |     -     \n",
      "   2    |   220   |   0.001437   |     -      |     -     |     -      |     -     \n",
      "   2    |   221   |   0.001241   |     -      |     -     |     -      |     -     \n",
      "   2    |    -    |   0.001992   |  0.001158  |  0.432892  |  0.348768  |  0.579419 \n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val F1   | Val Precision | Val Recall\n",
      "   3    |   20    |   0.001362   |     -      |     -     |     -      |     -     \n",
      "   3    |   40    |   0.001254   |     -      |     -     |     -      |     -     \n",
      "   3    |   60    |   0.001176   |     -      |     -     |     -      |     -     \n",
      "   3    |   80    |   0.001210   |     -      |     -     |     -      |     -     \n",
      "   3    |   100   |   0.001076   |     -      |     -     |     -      |     -     \n",
      "   3    |   120   |   0.001060   |     -      |     -     |     -      |     -     \n",
      "   3    |   140   |   0.001081   |     -      |     -     |     -      |     -     \n",
      "   3    |   160   |   0.001033   |     -      |     -     |     -      |     -     \n",
      "   3    |   180   |   0.000933   |     -      |     -     |     -      |     -     \n",
      "   3    |   200   |   0.001023   |     -      |     -     |     -      |     -     \n",
      "   3    |   220   |   0.000876   |     -      |     -     |     -      |     -     \n",
      "   3    |   221   |   0.000839   |     -      |     -     |     -      |     -     \n",
      "   3    |    -    |   0.001099   |  0.000759  |  0.436037  |  0.351920  |  0.582197 \n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val F1   | Val Precision | Val Recall\n",
      "   4    |   20    |   0.000885   |     -      |     -     |     -      |     -     \n",
      "   4    |   40    |   0.000884   |     -      |     -     |     -      |     -     \n",
      "   4    |   60    |   0.000820   |     -      |     -     |     -      |     -     \n",
      "   4    |   80    |   0.000843   |     -      |     -     |     -      |     -     \n",
      "   4    |   100   |   0.000878   |     -      |     -     |     -      |     -     \n",
      "   4    |   120   |   0.000729   |     -      |     -     |     -      |     -     \n",
      "   4    |   140   |   0.000812   |     -      |     -     |     -      |     -     \n",
      "   4    |   160   |   0.000799   |     -      |     -     |     -      |     -     \n",
      "   4    |   180   |   0.000715   |     -      |     -     |     -      |     -     \n",
      "   4    |   200   |   0.000777   |     -      |     -     |     -      |     -     \n",
      "   4    |   220   |   0.000816   |     -      |     -     |     -      |     -     \n",
      "   4    |   221   |   0.000475   |     -      |     -     |     -      |     -     \n",
      "   4    |    -    |   0.000813   |  0.000648  |  0.434051  |  0.348313  |  0.581954 \n",
      "\n",
      "\n",
      " Epoch  |  Batch  |  Train Loss  |  Val Loss  |  Val F1   | Val Precision | Val Recall\n",
      "   5    |   20    |   0.000766   |     -      |     -     |     -      |     -     \n",
      "   5    |   40    |   0.000752   |     -      |     -     |     -      |     -     \n",
      "   5    |   60    |   0.000723   |     -      |     -     |     -      |     -     \n",
      "   5    |   80    |   0.000656   |     -      |     -     |     -      |     -     \n",
      "   5    |   100   |   0.000707   |     -      |     -     |     -      |     -     \n",
      "   5    |   120   |   0.000772   |     -      |     -     |     -      |     -     \n",
      "   5    |   140   |   0.000678   |     -      |     -     |     -      |     -     \n",
      "   5    |   160   |   0.000726   |     -      |     -     |     -      |     -     \n",
      "   5    |   180   |   0.000732   |     -      |     -     |     -      |     -     \n",
      "   5    |   200   |   0.000753   |     -      |     -     |     -      |     -     \n",
      "   5    |   220   |   0.000619   |     -      |     -     |     -      |     -     \n",
      "   5    |   221   |   0.000649   |     -      |     -     |     -      |     -     \n",
      "   5    |    -    |   0.000717   |  0.000620  |  0.436188  |  0.351275  |  0.582689 \n",
      "\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)    # Set seed for reproducibility\n",
    "bert_classifier, optimizer, scheduler = initialize_model(epochs=5)\n",
    "train(bert_classifier,optimizer, scheduler, train_loader, val_loader, epochs=5, evaluation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "30b89b6e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T18:24:10.742556Z",
     "start_time": "2021-10-24T18:20:53.072005Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loss, test_f1, test_prec, test_recall = evaluate(bert_classifier, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "c38cf287",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T18:24:39.354821Z",
     "start_time": "2021-10-24T18:24:39.350225Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0005686048640899743,\n",
       " 0.4421930938862923,\n",
       " 0.35645496977896557,\n",
       " 0.5888542727697139)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss, test_f1, test_prec, test_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f76fca89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T18:25:10.980142Z",
     "start_time": "2021-10-24T18:25:06.929869Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = iter(test_loader).next()\n",
    "test = bert_classifier(batch[0], batch[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8238a80c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T18:25:11.781000Z",
     "start_time": "2021-10-24T18:25:11.775186Z"
    }
   },
   "outputs": [],
   "source": [
    "haha = np.array(test>0.5, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "918f1c3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T18:25:12.772951Z",
     "start_time": "2021-10-24T18:25:12.765347Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "40057e73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-24T18:25:15.555762Z",
     "start_time": "2021-10-24T18:25:15.545222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "haha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9478c633",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b76b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b3c850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
